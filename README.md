The notebook provided shows steps performed in ECG data classification.
Firstly, the data was pre-processed using balancing and augmentation followed by signal filtering using Gaussian filter.
Secondly, the effect of two different approaches (1D-CNN and LSTM-RNN) were investigated in performing beat classification.
Hyperparameters of 1D-CNN model were fine-tuned using Optuna module and was skipped for LSTM-RNN model owing to high compuatational time.
Thereafter, a hybrid CNN-LSTM model was developed to perform classification which achieved overall 95% test accuracy and over 90% for precision, recall and f1-score for all class labels.

The final notebook is CNN-LSTM.ipynb


The EU AI Act Compliance Checker results are as follows:

Your results
Deployer obligations
As a deployer of a high-risk AI system, you must comply with Article 26 obligations.
These include:
•	Taking appropriate technical and organisational measures to ensure they use such systems in accordance with the instructions of use accompanying the systems.
•	Monitoring the operation of the system on the basis of the instructions of use and when relevant, informing providers in accordance.
•	Prior to installing or using the system within the workplace, consulting workers representatives with a view to reaching an agreement in accordance with Directive 2002/14/EC and informing the affected employees that they will be subject to the system.
•	Cooperating with the relevant national competent authorities on any action those authorities take in relation with the high-risk system in order to implement the Regulation.
To the extent that you exercise control over the system, you must:
•	Implement human oversight, ensuring that the person or persons assigned to ensure this oversight are competent, properly qualified and trained, and have the necessary resources in order to ensure effective supervision.
•	Ensure that relevant and appropriate robustness and cybersecurity measures are regularly monitored for effectiveness and are regularly adjusted or updated.
•	Ensure that input data is relevant and sufficiently representative in view of the intended purpose of the high-risk AI system.
•	Keep logs automatically generated by the system for at least six months.
Where the system makes decisions or assists in decision-making related to people (such as in hiring or education), you must inform those people:
•	That they are subject to the use of the high-risk AI system.
•	Of the system's intended purpose and the type of decisions it makes.
•	About their right to an explanation.
If you believe that using the AI system according to its instructions might harm the health, safety, or rights of any person, you must:
•	Without undue delay, inform the provider or distributor and relevant national supervisory authorities.
•	Suspend the use of the system.
If you have identified any serious incident or any malfunctioning, you must:
•	Interrupt the use of the AI system.
•	Immediately inform first the provider, and then the importer or distributor and relevant national supervisory authorities.
High-risk obligations
Under Article 6, high-risk obligations apply to systems that are considered a 'safety component' of the kind listed in Annex I Section A, and to systems that are considered a 'High-risk AI system' under Annex III.
You need to follow these obligations for high-risk systems:
•	Establish and implement risk management processes according to Article 9.
•	Use high-quality training, validation and testing data according to Article 10.
•	Establish documentation and design logging features according to Article 11 and Article 12.
•	Ensure an appropriate level of transparency and provide information to users according to Article 13.
•	Ensure human oversight measures are built into the system and/or implemented by users according to Article 14.
•	Ensure robustness, accuracy and cybersecurity according to Article 15.
•	Set up a quality management system according to Article 17.
Excluded: Research & development
AI systems and models with the sole purpose of scientific research and development are excluded. For all other systems, research & development activities are likely excluded until your AI system is placed on the market or put into service. Systems and activities that are excluded are not subject to any obligations. For more information see Article 2 points 6 and 8.
Definitions for these results
Place on the market: the first making available of an AI system or a general purpose AI model on the Union market.
Put into service: the supply of an AI system for first use directly to the deployer or for own use in the Union for its intended purpose.
